{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsJcbyVeQqFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip po_fre_xml_cmu.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eks6tRgNNfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_PR7qVb69LG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install curl autoconf automake libtool pkg-config\n",
        "!git clone https://github.com/openvenues/libpostal\n",
        "%cd libpostal\n",
        "!./bootstrap.sh\n",
        "!./configure --datadir=/content\n",
        "!make -j4\n",
        "!sudo make install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6heCkWI68W9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo ldconfig\n",
        "!pip install postal\n",
        "from postal.parser import parse_address\n",
        "!pwd\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqnU17RFjS-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xml.etree.ElementTree as ET \n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn6RJDCPKcAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_yHkSd6KT4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.load_model('address_detection_bilstm.h5')\n",
        "with open(\"address_detection_bilstm_tokenizer.pickle\", \"rb\") as output_file:\n",
        "    tokenizer = pickle.load(output_file)\n",
        "\n",
        "    print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlcFJAcRNh0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# str1 - company name, str2 - address that contains company name\n",
        "def string_matcher(str1, str2):\n",
        "    words = str1.split(' ')\n",
        "    cnt = 0\n",
        "    for word in words:\n",
        "        # print(word)\n",
        "        if fuzz.token_set_ratio(word, str2) == 100:\n",
        "            # print(fuzz.partial_ratio(word, line[1]))\n",
        "            cnt += 1\n",
        "\n",
        "    if cnt / len(words) > 0.5:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdAKTPPWATIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_address(row):\n",
        "    address = list()\n",
        "    count = 0\n",
        "    address_list = list()\n",
        "    address_new = \"\"\n",
        "    po_list = list()\n",
        "    for item in row:\n",
        "        if item[1] != \"house\":\n",
        "            if item[1] == \"postcode\":\n",
        "                count +=1\n",
        "            address.append (item)\n",
        "        elif item[1] == \"house\":\n",
        "            index1 = row.index(item)\n",
        "            if index1 == 0:\n",
        "                continue\n",
        "            elif index1 == len(row) - 1:\n",
        "                continue\n",
        "            else:\n",
        "                if row[index1-1][1] != \"house\" and row[index1+1] != \"house\":\n",
        "                    address.append (item)\n",
        "            \n",
        "    if count > 1:\n",
        "        for item in address:\n",
        "            if item[1] != \"postcode\":\n",
        "                address_new = address_new + item[0] + \" \"\n",
        "            else:\n",
        "                address_new = address_new + item[0] + \" \"\n",
        "                index1 = address.index (item)\n",
        "                if index1 == len(address)-1:\n",
        "                    address_list.append (address_new)\n",
        "                    continue\n",
        "                else:\n",
        "                    if address[index1+1][1] == \"country\":\n",
        "                        address_new = address_new + address[index1+1][0] + \" \"\n",
        "                address_list.append (address_new)\n",
        "                address_new = \"\"\n",
        "    elif count == 1:\n",
        "        for item in address:\n",
        "            address_new = address_new + item[0] + \" \"\n",
        "        address_list.append (address_new)\n",
        "    \n",
        "    else:\n",
        "        address_list.append (address_new)\n",
        "        \n",
        "    if address_list == ['']:\n",
        "        return np.nan\n",
        "    else:\n",
        "        return address_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_XDdO4o0kKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GenerateAddressEntitiesText(text_data):\n",
        "    list_entities = parse_address(text_data)\n",
        "    list_transformed_text = list()\n",
        "    for text, entity in list_entities:\n",
        "        list_transformed_text.append('[%s]' % entity)\n",
        "    return ' '.join(list_transformed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GrcyLf7KU58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def address_detect (df):\n",
        "    testing_values = df['text_transformed'].values\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_values)\n",
        "    testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
        "    testing_predicted_probs = model.predict(testing_padded)\n",
        "    \n",
        "    df['pred_sigmoid'] = testing_predicted_probs\n",
        "    df['pred_class'] = df['pred_sigmoid'].apply(lambda x: 1 if x>=0.5 else 0)\n",
        "    \n",
        "    data_address = df[df[\"pred_class\"] == 1]\n",
        "    data_address = data_address.reset_index (drop = True)\n",
        "    \n",
        "    data_address[\"Parsed\"] = data_address[\"text\"].apply (lambda x: parse_address(str(x)))\n",
        "    data_address[\"Address\"] = data_address[\"Parsed\"].apply (lambda x: get_address(x))\n",
        "    data_address = data_address.dropna(subset=['Address'])\n",
        "    data_address = data_address.reset_index (drop = True)\n",
        "    \n",
        "    return data_address"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXUVbVWI6W2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS7DbCd8eOYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnt1 = 0\n",
        "cnt2 = 0\n",
        "cnt3 = 0\n",
        "total = 0\n",
        "for xmlfile in os.listdir(\"/content/po_fre_xml_cmu/po_fre_xml_cmu/train\"):\n",
        "    total += 1\n",
        "    \n",
        "    print(xmlfile + \":\")\n",
        "    print(total)\n",
        "    print(cnt1)\n",
        "    print(cnt2)\n",
        "    print(cnt3)\n",
        "\n",
        "    with open(\"/content/error_xml.txt\", \"r\") as error_xml:\n",
        "        error_xml = error_xml.read().splitlines()\n",
        "        # if total < int(error_xml[0]):\n",
        "        #     continue\n",
        "        if xmlfile in error_xml:\n",
        "            continue\n",
        "\n",
        "    address_key_file = \"true_address_key_case_sen.txt\"\n",
        "    with open(\"/content/po_fre_xml_cmu/po_fre_xml_cmu/train/\" + xmlfile,'r') as xml:\n",
        "        xml = xml.readlines()\n",
        "        xml[1] = \"<document>\\n\"\n",
        "\n",
        "    with open(\"/content/po_fre_xml_cmu/po_fre_xml_cmu/train/\" + xmlfile,'w') as xml2:\n",
        "        xml = ''.join(xml)\n",
        "        xml2.write(xml)\n",
        "\n",
        "    tree = ET.parse(\"/content/po_fre_xml_cmu/po_fre_xml_cmu/train/\" + xmlfile)\n",
        "    root = tree.getroot() \n",
        "\n",
        "    with open(xmlfile+'.parsed','w') as output:\n",
        "\n",
        "        for block_item in root.findall(\".//*/block\"):\n",
        "            block_str = ''\n",
        "            for line_item in block_item.findall(\".//*/formatting\"):\n",
        "                if line_item.text is not None: # To Solve the bug - TypeError: 'NoneType' object is not iterable\n",
        "                    if ':' in list(line_item.text):\n",
        "                        if block_str != '':\n",
        "                            block_str += '\\t' + block_item.attrib['l'] + '\\t' + block_item.attrib['t'] + '\\t' + block_item.attrib['r'] + '\\t' + block_item.attrib['b'] + '\\n'\n",
        "                    block_str += line_item.text+' '\n",
        "            if block_str != '':\n",
        "                output.write(block_str + '\\t' + block_item.attrib['l'] + '\\t' + block_item.attrib['t'] + '\\t' + block_item.attrib['r'] + '\\t' + block_item.attrib['b'] + '\\n')\n",
        "\n",
        "\n",
        "    with open(xmlfile+'.parsed','r') as parsed_xml:\n",
        "        parsed_xml = parsed_xml.readlines()\n",
        "        with open('should_be_address_or_not_item.txt', 'w') as output:\n",
        "            for item in parsed_xml:\n",
        "                if len(item.split('\\t')) < 5: #To Solve the List out of Range Error\n",
        "                    break\n",
        "                b = item.split('\\t')[4].strip()\n",
        "                item = item.split('\\t')[0].strip()\n",
        "                with open(address_key_file, 'r') as address_key:\n",
        "                  address_key = address_key.readlines()\n",
        "                  for key in address_key:\n",
        "                      key = key.strip()\n",
        "                      if string_matcher(key, item) and len(item) < 140: # filter out the text that is too long\n",
        "                          output.write(item + '\\t' + b + '\\n')\n",
        "                          break\n",
        "\n",
        "    data = pd.read_csv('should_be_address_or_not_item.txt', sep='\\t', header=None, names=['text', 'position'])\n",
        "    data['text_transformed'] = data['text'].apply(lambda x: GenerateAddressEntitiesText(x))\n",
        "    data_address = address_detect(data)\n",
        "    data_address.to_csv(r'/content/output.csv', index = False)\n",
        "\n",
        "\n",
        "    address_candidate = data_address['text'].values.tolist()\n",
        "    position_data = data_address[['text', 'position']].values.tolist()\n",
        "    output = []\n",
        "    for item in address_candidate:\n",
        "        output.append(item)\n",
        "\n",
        "    with open('should_be_address_or_not_item.txt', 'r') as input:\n",
        "        input = input.readlines()\n",
        "        flag_cnt1 = False\n",
        "        flag_cnt2 = False\n",
        "        for item in input:\n",
        "            item = item.strip().split('\\t')\n",
        "            if item[0] not in address_candidate: # Special cases: 1. Company name. 2. See below\n",
        "                if string_matcher('below', item[0].lower()) or string_matcher('above', item[0].lower()) or string_matcher('previous', item[0].lower()):\n",
        "                    flag_cnt2 = True\n",
        "\n",
        "                    cnt2 += 1\n",
        "                    below_b = float(item[1])\n",
        "                    below_candidate = []\n",
        "                    for candidate, candidate_b in position_data:\n",
        "                        if candidate_b < below_b:\n",
        "                            below_candidate.append(candidate) # ideally, only append address whose key matches the below's key, see code for above\n",
        "                    output.append(item[0] + ' ' + ''.join(below_candidate))\n",
        "                elif string_matcher('above', item[0].lower()) or string_matcher('previous', item[0].lower()):\n",
        "                    cnt2 += 1\n",
        "                    above_b = float(item[1])\n",
        "                    above_candidate = []\n",
        "                    for candidate, candidate_b in position_data:\n",
        "                        if candidate_b > above_b:\n",
        "                            address_candidate.append(candidate)\n",
        "                    output.append(item[0] + ' ' + ''.join(above_candidate))\n",
        "                else:\n",
        "                    for candidate in address_candidate:\n",
        "                        if string_matcher(item[0], candidate): # Company name.\n",
        "                            flag_cnt1 = True\n",
        "                            output.append(item[0])  # We need the original company name's key + the real matched address\n",
        "                            break\n",
        "    if flag_cnt1:\n",
        "        cnt1 += 1\n",
        "    if flag_cnt2:\n",
        "        cnt2 += 1\n",
        "    print(len(address_candidate))\n",
        "    print(address_candidate)\n",
        "    print(len(output))\n",
        "    print(output)\n",
        "    if len(output) == 0:\n",
        "        cnt3 += 1\n",
        "print(\"Company Name Percentage: \" + str(cnt1 / total))\n",
        "print(\"Above or Below Percentage: \" + str(cnt2 / total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}